{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "Decision tree is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator in input variables.\n",
    "\n",
    "\n",
    "### Terminology\n",
    "\n",
    "* Root Node : It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "* Splitting :  It is a process of dividing a node into two or more sub-nodes.\n",
    "* Decision Node : When a sub-node splits into further sub-nodes\n",
    "* Leaf/ Terminal Node: Nodes do not split or the last node.\n",
    "* Pruning: When we remove sub-nodes of a decision node or can say opposite process of splitting.\n",
    "    **Pruning is one of the technique used tackle overfitting.**\n",
    "![title](Capture10.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b10afa47-4287-499f-803f-a2b903c09c3a"
    }
   },
   "source": [
    "Using the decision algorithm, we start at the tree root and split the data on the feature that results in the largest information gain (IG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "931e584c-ab69-4f83-ae26-b0032860065c"
    }
   },
   "source": [
    "In an iterative process, we can then repeat this splitting procedure at each child node until the leaves are **pure**. This means that the samples at each node all belong to the **same class**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "78dcb4af-4780-42db-9b8a-08f63f09cdbb"
    }
   },
   "source": [
    "In practice, this can result in a very deep tree with many nodes, which can easily lead to **overfitting**. Thus, we typically want to **prune** the tree by setting a limit for the maximal depth of the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a3f6b6a9-0710-4509-bf04-ffd87d2d9457"
    }
   },
   "source": [
    "### Maximizing information gain\n",
    "\n",
    "In order to split the nodes for most informative features, we need to define an objective function that we want to optimize. Here,our objective function is to maximize the information gain at each split, which we define as follows:\n",
    "\n",
    "![title](Capture.PNG)\n",
    "\n",
    "**f** - is the feature to perform the split\n",
    "\n",
    "**$D_p$** - dataset of the parent\n",
    "\n",
    "**$D_j$** - dataset of the child of node *j*\n",
    "\n",
    "**I** - Impurity Measure\n",
    "\n",
    "**$N_p$** - total number of samples at the parent node\n",
    "\n",
    "**$N_j$** - number of samples in *j*th child node\n",
    "\n",
    "So, Information gain is simply the difference between the impurity of the parent node ***I($D_p$)***  and the sum of the child node impurities.\n",
    "\n",
    "**lower the impurity** of the child nodes, the larger the **information gain**.\n",
    "\n",
    "for simplicity and to reduce the combinatorial search space, most libraries (including scikit-learn) implement binary decision trees. This means that each parent node is split into two child nodes, D$_l$$_e$$_f$$_t$ and D$_r$$_i$$_g$$_h$$_t$ :\n",
    "![title](Capture1.PNG)\n",
    "\n",
    "There are three **impurity measures** or splitting criteria that are commonly used in binary decision trees are: \n",
    "\n",
    "1. **Entropy  ($I_H$)**\n",
    "\n",
    "2. **Gini index  ($I_G$)**\n",
    "\n",
    "3. **Classification error  ($I_E$)**.\n",
    "\n",
    "Let's start with the definition of **Entropy** for all non-empty classes\n",
    "![title](Capture6.PNG)\n",
    "A graph of entropy is given in fig.\n",
    "\n",
    "Suppose that we have +ve and -ve examples of some features. If all the examples are +ve, than we don't get any extra information from knowing the value of feature since, whatever the value of feature, the example will be positive.Thus the entropy of that feature is 0. However , if feature separates the example into 50% positive and 50% negative, then the amount of entropy is at a maximum.(and that feature is very important to us)\n",
    "\n",
    "![title](Capture2.PNG)\n",
    "\n",
    "Here, ***p(i/t)*** is the proportion of the samples that belongs to class c for a particular node t. The entropy is therefore 0 if all samples at a node belong to the same class, and the entropy is maximal if we have a uniform class distribution.\n",
    "\n",
    "Therefore, we can say that the entropy criterion attempts to maximize the mutual information in the tree.\n",
    "\n",
    "Intuitively, the **Gini index** can be understood as a criterion to minimize the probability of misclassification:\n",
    "![title](Capture3.PNG)\n",
    "\n",
    "However, in practice both the Gini index and entropy typically yield very similar results and it is often not worth spending much time on evaluating trees using different impurity criteria rather than experimenting with different pruning cut-offs.\n",
    "\n",
    "Another impurity measure is the **Classification error**:\n",
    "![title](Capture4.PNG)\n",
    "\n",
    "This is a useful criterion for pruning but not recommended for growing a decision tree, since it is less sensitive to changes in the class probabilities of the nodes.\n",
    "\n",
    "Ex: Lets take a example ![title](Capture5.PNG)\n",
    "\n",
    "We start with a dataset $D_p$ at the parent node $D_p$ that consists of 40 samples from class 1 and 40 samples from class 2 that we split into two datasets D$_l$$_e$$_f$$_t$ and D$_r$$_i$$_g$$_h$$_t$,respectively. The information gain using the **classification error** as a splitting criterion would be the same **(IG$_E$ = 0.25)** in both scenario A and B:\n",
    "\n",
    "I$_E$(D$_P$) = 1 - 0.5 = 0.5\n",
    "\n",
    "A : I$_E$(D$_l$$_e$$_f$$_t$) = 1 - 3/4 = 0.25\n",
    "\n",
    "A : I$_E$(D$_r$$_i$$_g$$_h$$_t$) = 1 - 3/4 = 0.25\n",
    "\n",
    "A ; $IG_E$ = 0.5 - (4/8)*0.25 - (4/8)*0.25 = 0.25\n",
    "\n",
    "**For B**\n",
    "\n",
    "B : I$_E$(D$_l$$_e$$_f$$_t$) = 1 - 4/6 = 1/3\n",
    "\n",
    "B : I$_E$(D$_r$$_i$$_g$$_h$$_t$) = 1 - 1 = 0\n",
    "\n",
    "B ; $IG_E$ = 0.5 - (6/8)*(1/3) - 0 = 0.25\n",
    "\n",
    "However, the **Gini index** would favor the split in scenario B(IG$_G$ = 0.16) over scenario, A(IG$_G$ = 0.125) which is indeed more pure:\n",
    "\n",
    "I$_G$(D$_P$) = 1 - (0.5$^2$ + 0.5$^2$)  = 0.5\n",
    "\n",
    "A : I$_G$(D$_l$$_e$$_f$$_t$) = 1 - ((3/4)$^2$ + (1/4)$^2$) = 0.375\n",
    "\n",
    "A : I$_G$(D$_r$$_i$$_g$$_h$$_t$) = 1 - ((1/4)$^2$ + (3/4)$^2$) = 0.375\n",
    "\n",
    "A ; $IG_G$ = 0.5 - (4/8)(0.375) - (4/8)(0.375) = 0.125\n",
    "\n",
    "**For B**\n",
    "\n",
    "B : I$_G$(D$_l$$_e$$_f$$_t$) = 1 - ((2/6)$^2$ + (4/6)$^2$) = 0.4\n",
    "\n",
    "B : I$_G$(D$_r$$_i$$_g$$_h$$_t$) = 1 - (1$^2$ + 0$^2$) = 0\n",
    "\n",
    "B ; $IG_G$ = 0.5 - (6/8)*(0.4) - 0 = 0.16\n",
    "\n",
    "Similarly, the **Entropy criterion** would favor scenario B(IG$_H$ = 0.19) over\n",
    "scenario A(IG$_H$ = 0.31):\n",
    "\n",
    "I$_H$(D$_P$) = - (0.5 log$_2$(0.5) + 0.5 log$_2$(0.5))  = 1\n",
    "\n",
    "A : I$_H$(D$_l$$_e$$_f$$_t$) = -((3/4) log$_2$(3/4) + (1/4) log$_2$(1/4)) = 0.81\n",
    "\n",
    "A : I$_H$(D$_r$$_i$$_g$$_h$$_t$) = -((1/4) log$_2$(1/4) + (3/4) log$_2$(3/4)) = 0.81\n",
    "\n",
    "A ; $IG_H$ = 1 - (4/8)(0.81) - (4/8)(0.81) = 0.19\n",
    "\n",
    "**For B**\n",
    "\n",
    "B : I$_H$(D$_l$$_e$$_f$$_t$) = -((2/6) log$_2$(2/6) + (4/6) log$_2$(4/6)) = 0.92\n",
    "\n",
    "B : I$_H$(D$_r$$_i$$_g$$_h$$_t$) = 0\n",
    "\n",
    "B ; $IG_H$ = 1 - (6/8)*(0.92) - 0 = 0.31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6aa80a14-dad5-49fd-9113-fad81191ce87"
    }
   },
   "source": [
    "## Dealing with Continuous Variables\n",
    "\n",
    "One thing that we have not yet discussed is how to deal with continuous variables, we have only considered those with discrete sets of feature values. The simplest solution is to diseretise the continuous variable. However, it is also possible to leave it continuous and modify the algorithm. For a continuous variable there is not just one place to split it: the variable can be broken between any pair of datapoints, as shown in fig.\n",
    "\n",
    "![title](Capture7.PNG)\n",
    "\n",
    "The trees that these algorithms make are all univariate trees, because they pick one feature (dimension) at a time and split according to that one. There are also algorithms that make multivariate trees by picking combinations of features. Fig. shows the idea. Given a dataset that contains three classes, the algorithm picks a feature and value for that feature to split the remaining data into two. \n",
    "\n",
    "![title](Capture8.PNG)\n",
    "\n",
    "\n",
    "As the outputs are continuous, so a regression model is appropriate. So, we will choose sum-of-squares-error.\n",
    "\n",
    "We can then pick the feature that has the split point that provides the best sum-of-squares error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints on Tree Size\n",
    "\n",
    "![title](Capture11.PNG)\n",
    "\n",
    "* Minimum samples for a node split\n",
    "* Minimum samples for a terminal node (leaf)\n",
    "* Maximum depth of tree (vertical depth)\n",
    "* Maximum number of terminal nodes\n",
    "* Maximum features to consider for split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Decision Tree in Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "\n",
    "model = tree.DecisionTreeClassifier(criterion='gini') \n",
    "\n",
    "#for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  \n",
    "#model = tree.DecisionTreeRegressor() for regression\n",
    "#Train the model using the training sets and check score\n",
    "\n",
    "model.fit(X, y)\n",
    "model.score(X, y)\n",
    "\n",
    "#Predict Output\n",
    "\n",
    "predicted= model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice feature in scikit-learn is that it allows us to export the decision tree as a .dot file after training, which we can visualize using the GraphViz program. This program is freely available at http://www.graphviz.org and supported by Linux, Windows, and Mac OS X.\n",
    "\n",
    "First, we create the .dot file via scikit-learn using the export_graphviz function from the tree submodule, as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree,out_file='tree.dot',feature_names=['petal length', 'petal width'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have installed GraphViz on our computer, we can convert the tree.dot file into a PNG file by executing the following command from the command line in the location where we saved the tree.dot file:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dot -Tpng tree.dot -o tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it will look like something this:\n",
    "![title](Capture12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d1678790-ba66-4008-86f1-a4f00a378858"
    }
   },
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "97f707b7-33b1-432b-a5f4-2690e4dd199c"
    }
   },
   "source": [
    "### Bagging \n",
    "\n",
    "The simplest method of combining classifiers is known as bagging, which stands for *bootstrap aggregating*, the statistical description of the method. \n",
    "\n",
    "A bootstrap sample is a sample taken from the original dataset with replace-ment, so that we may get some data several times and others not at all.\n",
    "\n",
    "The bootstrap sample is the same size as the original, and lots and lots of these samples are taken: B of them, where B is at least 50, and could even be in the thousands.\n",
    "\n",
    "![title](Capture13.PNG)\n",
    "\n",
    "\n",
    "* Bagging puts most of its effort into ensuring that different classifiers see different data( different samples of data) which is different than boosting where data stays the same but its importance changes with different classifiers.\n",
    "\n",
    "* Here the number of models built is not a hyper-parameters. Higher number of models are always better or may give similar performance than lower numbers. It can be theoretically shown that the variance of the combined predictions are reduced to 1/n (n: number of classifiers) of the original variance, under some assumptions.\n",
    "\n",
    "* **Random forest** is one of them.\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "* In Random Forest, we grow multiple trees as opposed to a single decision tree.\n",
    "* To classify a new object based on attributes, each tree gives a classification and we say the tree “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees.\n",
    "* Each tree is grown to the largest extent possible and  there is no pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Random Forest in Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier #use RandomForestRegressor for regression problem\n",
    "\n",
    "#X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create Random Forest object\n",
    "\n",
    "model= RandomForestClassifier(n_estimators=1000)\n",
    "\n",
    "# Train the model using the training sets and check score\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "#Predict Output\n",
    "\n",
    "predicted= model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "If we take a collection of very poor learners, each performing only just better than chance, then by putting them together it is possible to make an ensemble learner that can perform arbritrarily well. So we just need lots of low quality learners, and a way to put them together usefully, and we can make a learner that will do very well. The principal algorithm of boosting is named *AdaBoost*.\n",
    "\n",
    "The algorithm was proposed as an improvement on the original 1990 boosting algorithm. In that algorithm, the training set was split into three. A classifier was trained on the first third, and then tested on the sec-ond third. All of the data that was misclassified during that testing was used to form a new dataset, along with an equally sized random selection of the data that was correctly classified. A second classifier was trained on this new dataset, and then both of the classifiers were tested on the final third of the dataset. If they both produced the same output, then that datapoint was ig-nored, otherwise the datapoint was added to yet another new dataset, which formed the training set for a third classifier\n",
    "\n",
    "### Adaboost\n",
    "\n",
    "The innovation that AdaBoost (which stands for adaptive boosting) uses is to give weights to each datapoint according to how difficult previous classifiers have found to get it correct. These weights are given to the classifier as part of the input when it is trained. \n",
    "\n",
    "At each iteration a new classifier is trained on the training set, with the weights that are applied to the training set for each datapoint being modified at each iteration accord-ing to how successfully that datapoint has been classified in the past. The weights are initially all set to the same value, 1/N, whore N is the number of datapoints in the training set.\n",
    "\n",
    "![title](Capture9.PNG)\n",
    "\n",
    "* As points are misclassified, so their weights increased in boosting(datapoint getting larger), which increase the *importance* of that data point, make the classifier to pay more attention to them.\n",
    "\n",
    "* Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules.\n",
    "\n",
    "* Two most commonly used algorithms i.e. **Gradient Boosting (GBM)** and **XGboost**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "1. Initialize the outcome\n",
    "\n",
    "2. Iterate from 1 to total number of trees\n",
    "\n",
    "  2.1 Update the weights for targets based on previous run (higher for the ones mis-classified)\n",
    "  \n",
    "  2.2 Fit the model on selected subsample of data\n",
    "  \n",
    "  2.3 Make predictions on the full set of observations \n",
    "  \n",
    "  2.4 Update the output with current results taking into account the learning rate\n",
    "  \n",
    "3. Return the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM Parameters\n",
    "\n",
    "* **Tree-Specific Parameters** : These affect each individual tree in the model.\n",
    "* **Boosting Parameters**: These affect the boosting operation in the model.\n",
    "* **Miscellaneous Parameters**: Other parameters for overall functioning.\n",
    "\n",
    "#### Tree-Specific Parameters\n",
    "1. **min_samples_split** : Defines the minimum number of samples which are required in a node to be considered for splitting.\n",
    "2. **min_samples_leaf** : Defines the minimum samples should required in a terminal node or leaf.\n",
    "3. **max_depth** : Defines the maximum depth of a tree.\n",
    "4. **max_leaf_nodes** : Defines the maximum number of terminal nodes or leaves in a tree.\n",
    "5. **max_features** : \n",
    "    * The number of features to consider while searching for a best split. These will be randomly selected.\n",
    "    * As a thumb-rule, square root of the total number of features works great but we should check upto 30-40% of the total number of features.\n",
    "    \n",
    "The parameters which we have considered so far will affect step 2.2, i.e. **model building**.\n",
    "\n",
    "#### Boosting Parameters\n",
    "1. **learning_rate** : This determines the impact of each tree on the final outcome (step 2.4). The learning parameter controls the magnitude of this change in the estimates. Lower values are generally preferred as they make the model robust.\n",
    "2. **n_estimators** : The number of sequential trees to be modeled (step 2).\n",
    "3. **subsample** : \n",
    "    * The fraction of observations to be selected for each tree. Selection is done by random sampling.\n",
    "    * Values slightly less than 1 make the model robust by reducing the variance.\n",
    "    * Typical values ~0.8 generally work fine but can be fine-tuned further.\n",
    "\n",
    "#### Miscellaneous Parameters\n",
    "1. **loss** : It refers to the loss function to be minimized in each split.\n",
    "2. **init** : This affects initialization of the output.\n",
    "3. **random_state** : The random number seed so that same random numbers are generated every time.\n",
    "4. **warm_start** : This parameter has an interesting application and can help a lot if used judicially.Using this, we can fit additional trees on previous fits of a model. It can save a lot of time and you should explore this option for advanced applications.\n",
    "5. **verbose** : The type of output to be printed when the model fits. The different values can be:\n",
    "        0: no output generated (default)\n",
    "        1: output generated for trees in certain intervals\n",
    "       >1: output generated for all trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for GBM in Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier #For Classification\n",
    "from sklearn.ensemble import GradientBoostingRegressor #For Regression\n",
    "#use GBM function\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "clf.fit(X_train, y_train)\n",
    "#Predict Output\n",
    "predicted= model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "We always need to find a way to find the best parameters given the training data. In order to do so, we need to define a so-called **objective function**,to measure the performance of the model given a certain set of parameters.\n",
    "\n",
    "        objective function  ~  Training Loss + Regularization\n",
    "\n",
    "A very important fact about objective functions is they must always contain two parts: **training loss** and **regularization**.\n",
    "![title](Capture14.PNG)\n",
    "\n",
    "where L is the training loss function, and Ω is the regularization term. The training loss measures how predictive our model is on training data. For example, a commonly used training loss is mean squared error.\n",
    "\n",
    "The **regularization term** controls the complexity of the model, which helps us to avoid overfitting.Lets see the fig.\n",
    "\n",
    "![title](Capture15.PNG)\n",
    "\n",
    "The general principle is we want both a simple and predictive model. The tradeoff between the two is also referred as bias-variance tradeoff in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Parameters\n",
    "\n",
    "* **General Parameters** : Guide the overall functioning\n",
    "* **Booster Parameters** : Guide the individual booster (tree/regression) at each step\n",
    "* **Learning Task Parameters** : Guide the optimization performed\n",
    "\n",
    "#### General Parameters\n",
    "\n",
    "1. **booster [default=gbtree]** : Select the type of model to run at each iteration. It has 2 options:\n",
    "                                    gbtree: tree-based models\n",
    "                                    gblinear: linear models\n",
    "2. **silent [default=0]** : When Silent mode is set to 1, i.e. no running messages will be printed.(0 recommended)\n",
    "3. **nthread [default to maximum number of threads available if not set]** : This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically.\n",
    "\n",
    "#### Booster Parameters\n",
    "\n",
    "1. **eta [default=0.3]** : Analogous to learning rate in GBM. Typical final values to be used: 0.01-0.2\n",
    "2. **min_child_weight [default=1]** : Defines the minimum sum of weights of all observations required in a child.\n",
    "3. **max_depth [default=6]** : same as GBM. Typical values: 3-10\n",
    "4. **max_leaf_nodes** : The maximum number of terminal nodes or leaves in a tree.\n",
    "5. **gamma [default=0]** : A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "6. **subsample [default=1]** : Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "7. **lambda [default=1]** : L2 regularization term on weights (analogous to Ridge regression). This used to handle the regularization part of XGBoost. It should be explored to reduce overfitting.\n",
    "8. **alpha [default=0]** : L1 regularization term on weight (analogous to Lasso regression). Can be used in case of very high dimensionality so that the algorithm runs faster when implemented.\n",
    "9. **scale_pos_weight [default=1]** : A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence.\n",
    "\n",
    "#### Learning Task Parameters\n",
    "\n",
    "1. **objective [default=reg:linear]** : This defines the loss function to be minimized. Mostly used values are:\n",
    "\n",
    "    * **binary:logistic** –logistic regression for binary classification, returns predicted probability (not class)\n",
    "    * **multi:softmax** –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "    * **multi:softprob** –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "\n",
    "2. **eval_metric [ default according to objective ]** : The metric to be used for validation data. The default values are rmse for regression and error for classification.Typical values are :\n",
    "\n",
    "    * **rmse** – root mean square error\n",
    "    * **mae** – mean absolute error\n",
    "    * **logloss** – negative log-likelihood\n",
    "    * **error** – Binary classification error rate (0.5 threshold)\n",
    "    * **merror** – Multiclass classification error rate\n",
    "    * **mlogloss** – Multiclass logloss\n",
    "    * **auc**: Area under the curve\n",
    "\n",
    "\n",
    "In Scikit-Learn the parameters names which will change are:\n",
    "\n",
    "                    eta –> learning_rate\n",
    "                    lambda –> reg_lambda\n",
    "                    alpha –> reg_alpha"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
